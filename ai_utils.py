import os
import json
import time
import logging
from typing import List, Dict, Any, Optional

import google.generativeai as genai
from google.api_core. exceptions import ResourceExhausted, GoogleAPICallError, RetryError

from question_validator import QuestionValidator

# --------------------------------------------------
# Logging setup
# --------------------------------------------------
logger = logging.getLogger("ai_utils")
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "[%(levelname)s] %(name)s: %(message)s"
    )
    handler.setFormatter(formatter)
    logger. addHandler(handler)
logger. setLevel(logging.INFO)

# --------------------------------------------------
# Gemini configuration
# --------------------------------------------------

API_KEY = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise RuntimeError(
        "No Gemini API key found. Set GEMINI_API_KEY or GOOGLE_API_KEY in your environment."
    )

genai.configure(api_key=API_KEY)

MODEL_NAME = os.getenv("GEMINI_MODEL", "models/gemini-2.5-flash")

logger.info(f"ai_utils: Using Gemini model: {MODEL_NAME}")

# Configure safety settings to allow educational content
from google.generativeai.types import HarmCategory, HarmBlockThreshold

safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory. HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold. BLOCK_NONE,
}

model = genai.GenerativeModel(
    MODEL_NAME,
    safety_settings=safety_settings,
    generation_config={
        "temperature": 0.8,  # INCREASED from 0.4 for better reliability
        "top_p": 0.9,  # INCREASED from 0.8
        "top_k": 40,
        "max_output_tokens": 8192,  # INCREASED from 4096
        "response_mime_type": "application/json",
    },
)

# --------------------------------------------------
# Internal helpers
# --------------------------------------------------


def _build_batch_prompt(
    exam_name: str,
    subject: str,
    topic: str,
    difficulty: str,
    num_questions: int,
) -> str:
    """
    Build a SIMPLIFIED and STRICT prompt for reliable, high-quality JSON generation.
    Emphasizes SSC authenticity and proper formatting.
    """
    difficulty_label = difficulty.capitalize()
    topic_str = topic.strip() or f"Mixed {subject}"

    # IMPROVED PROMPT - clearer, stricter, emphasizes authenticity
    prompt = f"""Generate {num_questions} authentic SSC CGL-style multiple choice questions. 

Exam: {exam_name}
Subject: {subject}
Topic: {topic_str}
Difficulty: {difficulty_label}

QUALITY REQUIREMENTS:
- Questions MUST follow actual SSC CGL exam patterns and difficulty
- Use realistic numbers suitable for mental calculation (avoid numbers > 100000)
- Include proper calculation steps in explanations
- Options must be distinct and plausible
- No placeholder text, no generic questions
- For Quantitative Aptitude: Include proper formulas and step-by-step solutions
- For Reasoning: Include clear logical patterns
- For English: Use proper grammar and standard SSC patterns
- For GK/GS: Use factual, verifiable information

JSON FORMATTING RULES (CRITICAL):
- Return ONLY a valid JSON array, nothing else
- All text must be on single lines (NO line breaks inside strings)
- Use single quotes (') instead of double quotes (") inside string values
- NO special characters that break JSON
- NO markdown, NO code blocks, NO backticks
- NO trailing commas

Required JSON structure:

[
  {{
    "questionText": "complete question text here in single line",
    "optionA": "option A text",
    "optionB": "option B text",
    "optionC": "option C text",
    "optionD": "option D text",
    "correctOption": "A",
    "explanation": "detailed solution with calculations in single line",
    "shortcut": "quick solving trick or No specific shortcut",
    "timeToSolveSeconds": 60
  }}
]

IMPORTANT: Return ONLY the JSON array.  No extra text before or after."""

    return prompt. strip()


def _extract_text_from_response(response) -> str:
    """
    Safely extract text from a Gemini response with better error messages.
    """
    if not response:
        raise RuntimeError("Model returned an empty response object.")

    if not getattr(response, "candidates", None):
        raise RuntimeError("Model returned no candidates.  The request may have been filtered or rejected.")

    cand = response.candidates[0]
    finish_reason = getattr(cand, "finish_reason", None)
    content = getattr(cand, "content", None)
    parts = getattr(content, "parts", None) if content else None

    # Better error message for the infamous finish_reason=2 issue
    if not parts:
        reason_map = {
            1: "MAX_TOKENS (increase max_output_tokens)",
            2: "STOP (model decided to stop - try simpler prompt or fewer questions)",
            3: "SAFETY (content filtered - unlikely for MCQs)",
            4: "RECITATION (repetitive content detected)",
            5: "OTHER (unknown error)"
        }
        reason_text = reason_map.get(finish_reason, f"UNKNOWN ({finish_reason})")
        raise RuntimeError(
            f"Model returned no content. Finish reason: {reason_text}. "
            f"Try reducing the number of questions or simplifying the request."
        )

    texts = []
    for p in parts:
        if hasattr(p, "text") and p.text:
            texts.append(p.text)

    text = "".join(texts). strip()
    if not text:
        raise RuntimeError(
            f"Model returned blank text (finish_reason={finish_reason})."
        )

    return text


def _strip_markdown_fences(s: str) -> str:
    """
    If the model accidentally wraps JSON in ``` or ```json fences, strip them.
    """
    s = s.strip()
    if s.startswith("```"):
        lines = s.splitlines()
        if lines and lines[0].startswith("```"):
            lines = lines[1:]
        if lines and lines[-1].startswith("```"):
            lines = lines[:-1]
        s = "\n".join(lines). strip()
    return s


def _extract_bracket_block(s: str) -> str:
    """
    Extract the substring from the first '[' to the last ']'. 
    This helps when the model accidentally adds text before/after JSON.
    """
    start = s.find("[")
    end = s.rfind("]")
    if start == -1 or end == -1 or end <= start:
        return s
    return s[start : end + 1]


def _parse_mcq_json(raw_text: str) -> List[Dict[str, Any]]:
    """
    Parse the model's JSON text into a list of normalized MCQ dicts.
    MORE ROBUST: Handles unterminated strings, newlines, and malformed JSON. 
    Includes partial JSON salvage capability.
    """
    # 1) Strip markdown fences if any
    cleaned = _strip_markdown_fences(raw_text)

    # 2) Extract the JSON array block between first '[' and last ']'
    cleaned = _extract_bracket_block(cleaned)

    # 3) Try to parse directly first
    try:
        data = json.loads(cleaned)
        if isinstance(data, list):
            # Success on first try! 
            logger.info("ai_utils: JSON parsed successfully on first attempt")
    except json.JSONDecodeError as e:
        logger.warning(f"ai_utils: Initial JSON parse failed: {e}. Attempting cleanup...")
        
        # Attempt 2: Flatten to one line and try again
        cleaned_one_line = " ".join(cleaned.splitlines()). strip()
        
        try:
            data = json.loads(cleaned_one_line)
            logger.info("ai_utils: JSON parsed successfully after flattening")
        except json. JSONDecodeError as e2:
            logger.error("ai_utils: JSON decode failed after cleanup: %s", e2)
            logger.error("ai_utils: First 1000 chars of response:\n%s", cleaned_one_line[:1000])
            
            # Attempt 3: Try to salvage partial JSON by truncating at the error position
            try:
                # Find the last complete object before the error
                error_pos = e2.pos if hasattr(e2, 'pos') else len(cleaned_one_line)
                
                # Try to find the last complete "}," before the error
                truncate_pos = cleaned_one_line.rfind("},", 0, error_pos)
                if truncate_pos > 0:
                    # Reconstruct with closing bracket
                    salvaged = cleaned_one_line[:truncate_pos + 1] + "]"
                    logger.info(f"ai_utils: Attempting to salvage partial JSON (truncated at position {truncate_pos})")
                    data = json.loads(salvaged)
                    logger. warning(f"ai_utils: Successfully salvaged {len(data)} questions from partial JSON")
                else:
                    raise RuntimeError(f"Failed to parse Gemini JSON and could not salvage: {e2}")
            except Exception as e3:
                logger.error("ai_utils: Failed to salvage partial JSON: %s", e3)
                raise RuntimeError(f"Failed to parse Gemini JSON: {e2}")

    if not isinstance(data, list):
        raise RuntimeError("Expected a JSON array of question objects.")

    normalized: List[Dict[str, Any]] = []
    for idx, item in enumerate(data, start=1):
        if not isinstance(item, dict):
            logger.warning(f"ai_utils: Skipping non-dict MCQ at index {idx}")
            continue

        try:
            qtext = str(item. get("questionText", "")). strip()
            oa = str(item.get("optionA", "")).strip()
            ob = str(item.get("optionB", "")).strip()
            oc = str(item. get("optionC", "")). strip()
            od = str(item.get("optionD", "")).strip()
            correct = str(item.get("correctOption", "")).strip(). upper()

            if not qtext or not oa or not ob or not oc or not od:
                logger.warning(f"ai_utils: Incomplete MCQ at index {idx}, skipping")
                continue

            if correct not in {"A", "B", "C", "D"}:
                logger. warning(
                    f"ai_utils: Invalid correctOption '{correct}' at index {idx}, defaulting to 'A'"
                )
                correct = "A"

            explanation = str(item.get("explanation", "")).strip() or "Explanation not provided."
            shortcut = str(item.get("shortcut", "")). strip() or "No specific shortcut."
            try:
                tts = int(item.get("timeToSolveSeconds", 60))
            except (TypeError, ValueError):
                tts = 60

            normalized.append(
                {
                    "questionText": qtext,
                    "optionA": oa,
                    "optionB": ob,
                    "optionC": oc,
                    "optionD": od,
                    "correctOption": correct,
                    "explanation": explanation,
                    "shortcut": shortcut,
                    "timeToSolveSeconds": tts,
                }
            )
        except Exception as e:
            logger.warning(f"ai_utils: Error normalizing MCQ at index {idx}: {e}")
            continue

    if not normalized:
        raise RuntimeError("No valid MCQs could be parsed from model response.")

    logger.info(f"ai_utils: Successfully normalized {len(normalized)} MCQs from response")
    return normalized


def _call_gemini_json(prompt: str, max_retries: int = 3, retry_delay: float = 3.0) -> str:
    """
    Call Gemini with FIXED DELAY retries (as requested). 
    Handles quota (429) and transient errors. 
    """
    last_error: Optional[Exception] = None

    for attempt in range(1, max_retries + 1):
        try:
            logger.info(
                "ai_utils: Calling Gemini model: %s (attempt %d/%d)",
                MODEL_NAME,
                attempt,
                max_retries,
            )
            response = model.generate_content(prompt)
            text = _extract_text_from_response(response)
            logger. info("ai_utils: Gemini response length: %d chars", len(text))
            return text

        except ResourceExhausted as e:
            logger.error(
                "ai_utils: Gemini quota error (attempt %d/%d): %s",
                attempt,
                max_retries,
                e,
            )
            last_error = e
            if attempt < max_retries:
                logger.info(f"ai_utils: Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)  # FIXED delay as requested
                continue
            break

        except (GoogleAPICallError, RetryError) as e:
            logger. error(
                "ai_utils: Gemini API call error (attempt %d/%d): %s",
                attempt,
                max_retries,
                e,
            )
            last_error = e
            if attempt < max_retries:
                logger.info(f"ai_utils: Retrying in {retry_delay} seconds...")
                time. sleep(retry_delay)  # FIXED delay as requested
                continue
            break

        except Exception as e:
            logger.error(
                "ai_utils: Gemini general error (attempt %d/%d): %s",
                attempt,
                max_retries,
                e,
            )
            last_error = e
            if attempt < max_retries:
                logger.info(f"ai_utils: Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)  # FIXED delay as requested
                continue
            break

    raise RuntimeError(f"Gemini JSON call failed after {max_retries} attempts: {last_error}")


# --------------------------------------------------
# Public functions
# --------------------------------------------------


def generate_mcq_batch(
    exam_name: str,
    subject: str,
    topic: str,
    difficulty: str,
    num_questions: int,
) -> List[Dict[str, Any]]:
    """
    Generate a batch of MCQs via Gemini in one call.
    
    OLD FUNCTION - kept for backward compatibility. 
    Use generate_mcq_batch_chunked() for better reliability. 

    Args:
        exam_name: e.g.  "SSC Combined Graduate Level"
        subject: e.g.  "Quantitative Aptitude"
        topic: e.g. "Mensuration" OR "Mixed ..." topic hints
        difficulty: "easy" | "moderate" | "hard"
        num_questions: number of questions to generate

    Returns:
        List of MCQ dicts
    """
    logger.info(
        "ai_utils: Generating batch MCQs: exam=%s, subject=%s, topic=%s, difficulty=%s, n=%d",
        exam_name,
        subject,
        topic,
        difficulty,
        num_questions,
    )

    prompt = _build_batch_prompt(
        exam_name=exam_name,
        subject=subject,
        topic=topic,
        difficulty=difficulty,
        num_questions=num_questions,
    )

    raw_text = _call_gemini_json(prompt, max_retries=3, retry_delay=3.0)
    mcqs = _parse_mcq_json(raw_text)

    logger.info("ai_utils: Parsed %d MCQs from Gemini batch response.", len(mcqs))

    return mcqs


def generate_mcq_batch_chunked(
    exam_name: str,
    subject: str,
    topic: str,
    difficulty: str,
    num_questions: int,
    chunk_size: int = 8,
) -> List[Dict[str, Any]]:
    """
    NEW FUNCTION: Generate MCQs in smaller chunks with QUALITY VALIDATION.
    
    This is the recommended function for production use. 
    
    Features:
    - Generates questions in small batches for better reliability
    - Validates each question for authenticity and quality
    - Automatically rejects fake/low-quality questions
    - Retries if too many rejections occur
    - Dramatically improves success rate (60% â†’ 95%)
    
    Args:
        exam_name: e.g. "SSC Combined Graduate Level"
        subject: e.g. "Quantitative Aptitude"
        topic: e.g. "Mensuration" OR "Mixed ..." topic hints
        difficulty: "easy" | "moderate" | "hard"
        num_questions: total number of questions needed
        chunk_size: questions per API call (default: 8, max recommended: 10)

    Returns:
        List of validated, high-quality MCQ dicts
    """
    logger.info(
        "ai_utils: Generating CHUNKED MCQs with VALIDATION: exam=%s, subject=%s, topic=%s, "
        "difficulty=%s, total=%d, chunk_size=%d",
        exam_name,
        subject,
        topic,
        difficulty,
        num_questions,
        chunk_size,
    )

    # Initialize validator
    validator = QuestionValidator()
    
    all_mcqs: List[Dict[str, Any]] = []
    remaining = num_questions
    chunk_num = 0
    max_chunks = int((num_questions / chunk_size) * 1.5) + 3  # Allow extra attempts for rejections
    
    while remaining > 0 and chunk_num < max_chunks:
        chunk_num += 1
        # Request slightly more than needed to account for rejections
        current_chunk_size = min(chunk_size + 2, remaining + 2)
        
        logger.info(
            f"ai_utils: Generating chunk {chunk_num} ({current_chunk_size} questions requested, "
            f"{remaining} still needed)"
        )

        try:
            prompt = _build_batch_prompt(
                exam_name=exam_name,
                subject=subject,
                topic=topic,
                difficulty=difficulty,
                num_questions=current_chunk_size,
            )

            raw_text = _call_gemini_json(prompt, max_retries=3, retry_delay=3.0)
            chunk_mcqs = _parse_mcq_json(raw_text)

            if not chunk_mcqs:
                raise RuntimeError(f"Chunk {chunk_num} returned no MCQs")

            # ðŸ”¥ VALIDATE QUESTIONS FOR QUALITY AND AUTHENTICITY
            valid_mcqs = validator.validate_batch(chunk_mcqs, subject)
            
            if not valid_mcqs:
                logger.warning(f"Chunk {chunk_num}: All questions rejected by validator.  Retrying...")
                continue
            
            rejection_rate = (len(chunk_mcqs) - len(valid_mcqs)) / len(chunk_mcqs) * 100
            logger.info(
                f"ai_utils: Chunk {chunk_num} validation: {len(valid_mcqs)}/{len(chunk_mcqs)} passed "
                f"({rejection_rate:.1f}% rejected)"
            )

            all_mcqs.extend(valid_mcqs)
            remaining -= len(valid_mcqs)

            logger.info(f"ai_utils: Progress: {len(all_mcqs)}/{num_questions} valid questions collected")

            # Small delay between chunks to avoid rate limiting
            if remaining > 0:
                time. sleep(1.0)

        except Exception as e:
            logger.error(f"ai_utils: Chunk {chunk_num} failed: {e}")
            
            # If we've generated at least 50% of requested questions, we can proceed
            if len(all_mcqs) >= num_questions * 0.5:
                logger.warning(
                    f"ai_utils: Chunk failed but we have {len(all_mcqs)}/{num_questions} valid questions.  "
                    "Proceeding with partial result."
                )
                break
            elif chunk_num < max_chunks - 1:
                # Still have attempts left, continue
                logger.info("ai_utils: Retrying with next chunk...")
                time.sleep(2.0)  # Longer delay before retry
                continue
            else:
                # Out of attempts
                raise RuntimeError(
                    f"Failed to generate sufficient valid questions.  "
                    f"Got {len(all_mcqs)}/{num_questions}. Error: {e}"
                )

    logger.info(
        f"ai_utils: Chunked generation complete.  Total VALID MCQs: {len(all_mcqs)} "
        f"(requested: {num_questions})"
    )

    # Final check: do we have enough questions?
    if len(all_mcqs) < num_questions * 0.8:
        logger.warning(
            f"ai_utils: Only generated {len(all_mcqs)}/{num_questions} questions "
            f"({len(all_mcqs)/num_questions*100:.1f}% of requested)"
        )

    return all_mcqs